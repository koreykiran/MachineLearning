{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dog breed Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file consists of CNN for the 1st module of the proposed approach. I.e. this the Binary class classifier.\n",
    "This file also contains the method to test the image, and serves as pipeline to our second module i.e our multi class classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_yaml\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "from keras.callbacks import History \n",
    "history = History()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "# Step 1 - Convolution\n",
    "# number of features - 32,64,128 , size of each feaature map (3,3)\n",
    "# Input shape -> 3 for colored, 2 for black and white, size - 64,64 \n",
    "#activation function - to remove the negative pixel values , to remove the linearity \n",
    "classifier.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(64, 64, 3)))\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(activation = 'relu', units = 128))\n",
    "# activation - softmax for more than 2 classes, sigmoid - 2 classes\n",
    "classifier.add(Dense(activation = 'sigmoid', units = 1))\n",
    "# Compiling the CNN\n",
    "# binary_class = binary_crossentropy, multi - categorical_crossentropy\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we are using Sequential class from Keras models to build the CNN. Here we have 2 layers of convolution and a layer of Max pooling for each convolution layer.\n",
    "\n",
    "We then flatten the nodes and feed it to the fully connected layers. we have 2 fully connected layers, one uses 'relu' as the activation function and other uses 'sigmoid'.\n",
    "\n",
    "We have chosen 'adam optimizer' as the optimizer and 'binary crossentropy' as the loss function.\n",
    "\n",
    "### Model Summary :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22700 images belonging to 2 classes.\n",
      "Found 1461 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Part 2 - Fitting the CNN to the images\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "# class_mode = binary or categorical, target_size = same as input_size mentioned in Convo2D step\n",
    "training_set = train_datagen.flow_from_directory('dataset/train',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "test_set = test_datagen.flow_from_directory('dataset/test',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {v: k for k, v in training_set.class_indices.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the model check point so that we can save our trained model.\n",
    "Important property to note here is that it stores only the best weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='bclass.hdf5', \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are all good to start training the model. Since it takes a lot of time to train (nearly 3hrs for 2 epochs), we train for 2 epochs at a time and keep repeating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "22700/22700 [==============================] - 8619s 380ms/step - loss: 0.1570 - acc: 0.9372 - val_loss: 0.3306 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.48688 to 0.33061, saving model to bclass.hdf5\n",
      "Epoch 2/2\n",
      "22700/22700 [==============================] - 5724s 252ms/step - loss: 0.0539 - acc: 0.9802 - val_loss: 0.0261 - val_acc: 0.9898\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33061 to 0.02609, saving model to bclass.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23dc349ad68>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 22700,\n",
    "                         epochs = 2,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 1461,callbacks=[checkpointer],verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "22700/22700 [==============================] - 7862s 346ms/step - loss: 0.0313 - acc: 0.9891 - val_loss: 0.0218 - val_acc: 0.9932\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.02609 to 0.02183, saving model to bclass.hdf5\n",
      "Epoch 2/2\n",
      "22700/22700 [==============================] - 8608s 379ms/step - loss: 0.0226 - acc: 0.9924 - val_loss: 0.0719 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23dc3d1e9e8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 22700,\n",
    "                         epochs = 2,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 1461,callbacks=[checkpointer],verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "22700/22700 [==============================] - 7196s 317ms/step - loss: 0.0177 - acc: 0.9939 - val_loss: 0.0342 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Epoch 2/2\n",
      "22700/22700 [==============================] - 6198s 273ms/step - loss: 0.0152 - acc: 0.9949 - val_loss: 0.0267 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23dc3d18ac8>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 22700,\n",
    "                         epochs = 2,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 1461,callbacks=[checkpointer],verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "22700/22700 [==============================] - 8173s 360ms/step - loss: 0.0124 - acc: 0.9958 - val_loss: 0.0338 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Epoch 2/2\n",
      "22700/22700 [==============================] - 8005s 353ms/step - loss: 0.0114 - acc: 0.9964 - val_loss: 0.0246 - val_acc: 0.9918\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23dc3483d68>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 22700,\n",
    "                         epochs = 2,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 1461,callbacks=[checkpointer,history],verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "22700/22700 [==============================] - 8494s 374ms/step - loss: 0.0109 - acc: 0.9965 - val_loss: 0.0368 - val_acc: 0.9898\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Epoch 2/2\n",
      "22700/22700 [==============================] - 8111s 357ms/step - loss: 0.0100 - acc: 0.9969 - val_loss: 0.0645 - val_acc: 0.9801\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23dc348a588>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 22700,\n",
    "                         epochs = 2,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 1461,callbacks=[checkpointer,history],verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "22700/22700 [==============================] - 6247s 275ms/step - loss: 0.0093 - acc: 0.9971 - val_loss: 0.0298 - val_acc: 0.9918\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Epoch 2/2\n",
      "22700/22700 [==============================] - 6296s 277ms/step - loss: 0.0092 - acc: 0.9973 - val_loss: 0.0279 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23dc3f73f60>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 22700,\n",
    "                         epochs = 2,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 1461,callbacks=[checkpointer,history],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "def save_bclass():\n",
    "    classifier_yaml = classifier.to_yaml()\n",
    "    with open(\"model.yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(classifier_yaml)\n",
    "    # serialize weights to HDF5\n",
    "    classifier.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "save_bclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "def load_bclass():\n",
    "    # load YAML and create model\n",
    "    yaml_file = open('model.yaml', 'r')\n",
    "    loaded_classifier_yaml = yaml_file.read()\n",
    "    yaml_file.close()\n",
    "    loaded_classifier = model_from_yaml(loaded_classifier_yaml)\n",
    "    # load weights into new model\n",
    "    loaded_classifier.load_weights(\"model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_classifier\n",
    "    \n",
    "loaded_classifier = load_bclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def isDog(path):\n",
    "    img = imread(path) #make sure that path_to_file contains the path to the image you want to predict on. \n",
    "    img = resize(img,(64,64),mode='constant')\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "\n",
    "    if(np.max(img)>1):\n",
    "        img = img/255.0\n",
    "\n",
    "    prediction = loaded_classifier.predict_classes(img)\n",
    "    print(class_labels[prediction[0][0]])\n",
    "    return prediction[0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not_dog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isDog('test_images/h1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not_dog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isDog('test_images/lemon.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isDog('test_images/a1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isDog('test_images/d1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isDog('test_images/d2.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
