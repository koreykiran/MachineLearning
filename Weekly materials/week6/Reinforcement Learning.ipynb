{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning \n",
    "\n",
    "\n",
    "### Spring 2018\n",
    "\n",
    "### Minwoo \"Jake\" Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# QUIZZES\n",
    "\n",
    "## PollEv.com/mjlee\n",
    "\n",
    "### 5 min for Review Quiz\n",
    "### 10 min for Preview Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "- Announcement\n",
    "    - project\n",
    "- Review\n",
    "- Overview of RL\n",
    "- Modeling Exercise \n",
    "- MDP\n",
    "- VI/PI\n",
    "- TD Learning\n",
    "- Next Topic: Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](http://webpages.uncc.edu/mlee173/teach/itcs6156/images/class/werber_ls.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](http://webpages.uncc.edu/mlee173/teach/itcs6156/images/class/pavlovs-dogs-mark-stivers.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning \n",
    "\n",
    "- Only feedback: Reward\n",
    "- delayed feedback\n",
    "- sequential \n",
    "- the effects of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applications\n",
    "\n",
    "<img src=\"http://webpages.uncc.edu/mlee173/teach/itcs6156/images/class/RLexamples.png\" width=600/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Terminology\n",
    "\n",
    "- Reward: $R_t$ is a scalar feedback signal at time $t$. It indicates how well agent is doing at the time. \n",
    "\n",
    "- State: $S_t$ represents what happens currently and next. \n",
    "\n",
    "- Action: $A_t$ is how an agent affects to the environment that can change the state. \n",
    "\n",
    "- Observation: $O_t$ is what an agent recognizes the world for the state $S_t$. \n",
    "\n",
    "- Policy: A function that determines an agent's behavior or a function that selects its action, $\\pi(S_t)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Robot Walking \n",
    "\n",
    "- What can be state?\n",
    "- What can be action?\n",
    "- What is reward? \n",
    "- What are the observations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stock Trading\n",
    "\n",
    "- What can be state?\n",
    "- What can be action?\n",
    "- What is reward? \n",
    "- What are the observations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practice more with modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    " <font color='blue' size=20>$ ( S, A, P, R, \\gamma ) $ </font>\n",
    "* $S$ : a finite set of states\n",
    "* $A$ : a finite set of actions\n",
    "* $P$ : a state transition probability\n",
    "* $P^a_{ss^\\prime} = P [ S_{t+1} = s^\\prime | S_t = s, A_t = a ]$\n",
    "* $R$ : a reward function\n",
    "* $\\gamma$ : a discount factor\n",
    "    $\\gamma \\in [0, 1]$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Control / Decision Problems\n",
    "\n",
    "<table border=\"0\">\n",
    "<tr>\n",
    "<td> <img src=\"http://webpages.uncc.edu/mlee173/teach/itcs6156/images/class/Tbots.jpg\" width=300 /> </td>\n",
    "<td> <img src=\"http://webpages.uncc.edu/mlee173/teach/itcs6156/images/class/humanoid.gif\" width=200 /> </td>\n",
    "<td> <img src=\"http://webpages.uncc.edu/mlee173/teach/itcs6156/images/class/racecar.gif\" width=300 /> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "* Interaction between an agent and an environment\n",
    "* Recognition of states\n",
    "* Taking actions to an environment\n",
    "* How to evaluate feedback\n",
    "* What is the $\\gamma$ term then? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling for Decision Problems\n",
    "\n",
    "* MDP is one of the way to simplify the problem\n",
    "* How does MDP simplify the modeling of decision problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " **Markov Property**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Definition: A state $S_t$ is Markov if and only if \n",
    "<br/><br/>\n",
    "$$P [ S_{t+1} | S_t ] = P[ S_{t+1} | S_1, ..., S_t ]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#   <font color=\"green\">The future is not dependent of the past when the current state is given.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MDP\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/21/Markov_Decision_Process_example.png\"\n",
    "width=500 />  \n",
    "<center> MDP with 3 states and 2 actions (wikipedia)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Chain, Markov Processes, Markov Reward Processes\n",
    "\n",
    "\n",
    "| Observability |   State    |  No Action     |   Action                |\n",
    "|---------------|:-----------|:---------------|:------------------------|\n",
    "|    **Full**   | Discrete   | Markov Chain   | Markov Decision Process |\n",
    "|               | Continuous | Markov Process |        ___              |\n",
    "|  **Partial**  |    ___     |      HMM       |        POMDP            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Process / Markov Chain\n",
    "\n",
    "<table border=\"0\">\n",
    " <col width=\"200\">\n",
    "<tr> <td>\n",
    "<font color='blue' size=\"5\">$ ( S, P ) $ </font>\n",
    "</td><td>\n",
    "![Markov Chain](https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Markovkate_01.svg/220px-Markovkate_01.svg.png)</td></tr>\n",
    "<table>\n",
    " \n",
    "* $S$ : a finite set of states\n",
    "* $P$ : a state transition probability\n",
    "   $$ P_{ss^\\prime} = P [ S_{t+1} = s^\\prime | S_t = s ] $$\n",
    "* A memoryless random process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# State Transition Matrix\n",
    "\n",
    "* The state transition probability from state $s$ to following state $s^\\prime$:  \n",
    " $$ P_{ss^\\prime} = P [ S_{t+1} = s^\\prime | S_t = s ] $$\n",
    "\n",
    "* State Transition matrix can be defined as:\n",
    "$$ P =  \\begin{pmatrix}\n",
    "  P_{11} & P_{12} & \\cdots & P_{1n} \\\\\n",
    "  P_{21} & P_{22} & \\cdots & P_{2n} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  P_{n1} & P_{n2} & \\cdots & P_{nn} \n",
    " \\end{pmatrix} $$\n",
    " where the sum of each row is 1.\n",
    "\n",
    "* similar to adjacency matrix with probability values for each entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Reward Processes\n",
    "\n",
    "<br/>\n",
    " <font color='blue' size=5>$ ( S, P, R, \\gamma ) $ </font>\n",
    " \n",
    "* $S$ : a finite set of states\n",
    "* $P$ : a state transition probability\n",
    "* <font color=\"red\">$R$ : a reward function </font>\n",
    "* <font color=\"red\">$\\gamma$ : a discount factor\n",
    "    $\\gamma \\in [0, 1]$ </red>   \n",
    "* <font color=\"green\">Markov chain with values</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Return\n",
    "\n",
    "* How do we evaluate MRP? \n",
    "    * Simply, the sum of rewards\n",
    "    * The return $G_t$ is defined as the total discounted reward at time-step $t$:\n",
    "    $$ G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Discounting Factor?\n",
    "\n",
    "* Why do we need $\\gamma$? \n",
    "    * The first option for the return, simple sum of rewards, goes infinity. \n",
    "        * mathematically inconvenient\n",
    "    * Uncertainty about the future\n",
    "    * A reward in the future cannot be worth as much as a current reward.\n",
    "        * In economics, considering inflation and obliteration, \\$ 100k in 10 years later is not worth enough as \\$ 100k now. \n",
    "    * 朝三暮四 (three for morning and four for evening) or framing effect \n",
    "    \n",
    "    \n",
    "* Choice of $\\gamma$\n",
    "    * nearsighted ($\\gamma = 0$) vs farsighted ($\\gamma = 1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Value Function\n",
    "\n",
    "* Expected return starting from state $s$\n",
    "    $$ V(s) = E [ G_t | S_t = s ] $$\n",
    "* The long-term value of state $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bellman Equation\n",
    "\n",
    "* bootstraping representation of the value function\n",
    "* immediate reward + discounted value of successive state\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "  V(s) &= E [ G_t | S_t = s ] \\\\\n",
    "       &= E [ R_{t+1} + \\gamma R_{t+2} + \\cdots | S_t = s ] \\\\\n",
    "       &= E [ R_{t+1} + \\gamma ( R_{t+2} + \\gamma R_{t+3} + \\cdots ) | S_t = s ] \\\\\n",
    "       &= E [ R_{t+1} + \\gamma G_{t+1} | S_t = s ] \\\\\n",
    "       &= E [ R_{t+1} + \\gamma V(s_{t+1}) | S_t= s ]\\\\\n",
    "      \\\\\n",
    "  V(s) &= R(s) + \\gamma \\sum_{s^\\prime \\in S} P_{ss^\\prime} V(s^\\prime)      \n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "in Matrices, \n",
    "$$\n",
    " \\begin{align}\n",
    "    V &= R + \\gamma PV \\\\\n",
    "    \\\\\n",
    "  \\begin{pmatrix}\n",
    "  V_1 \\\\\n",
    "  V_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  V_n  \n",
    " \\end{pmatrix} &= \n",
    "  \\begin{pmatrix}\n",
    "  R_1 \\\\\n",
    "  R_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  R_n  \n",
    " \\end{pmatrix} +   \n",
    " \\gamma\n",
    " \\begin{pmatrix}\n",
    "  P_{11} & P_{12} & \\cdots & P_{1n} \\\\\n",
    "  P_{21} & P_{22} & \\cdots & P_{2n} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  P_{n1} & P_{n2} & \\cdots & P_{nn} \n",
    " \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "  V_1 \\\\\n",
    "  V_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  V_n  \n",
    " \\end{pmatrix}\n",
    " \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Solving the Bellman Equation\n",
    "\n",
    "* Direct solution:\n",
    "    $$ V = (I - \\gamma P)^{-1} R $$\n",
    "    * Cubic computational complexity\n",
    "    <br/><br/>\n",
    "* Iterative Solutions:\n",
    "    * Dynamic Programming\n",
    "    * Monte-Carlo Evaluation\n",
    "    * Temporal-Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Now, back to MDP\n",
    "\n",
    "* Inclusion of __actions__ that affects the state transitions\n",
    "<br/><br/>\n",
    " <font size=6>$ ( S, $<font color='red'>$A$</font>$, P, R, \\gamma )$  </font>\n",
    " <br/><br/>\n",
    "* $S$ : a finite set of states\n",
    "* <font color='red'>$A$ : a finite set of actions</font>\n",
    "* $P$ : a state transition probability <br/>\n",
    "     <font color='red'> $P^a_{ss^\\prime} = P [ S_{t+1} = s^\\prime | S_t = s, A_t = a ]$</font>\n",
    "* $R$ : a reward function <br/>\n",
    "     <font color='red'> $R^a_{s} = E [ R_{t+1} | S_t = s, A_t = a ]$</font>\n",
    "* $\\gamma$ : a discount factor $(\\gamma \\in [0, 1])$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Policy\n",
    "\n",
    "* A policy maps from a state to an action. \n",
    "\n",
    "|  State  |   Action  |          |  State  |   Action  |\n",
    "|:--------|:----------|:---------|:--------|:----------|\n",
    "|  $S_0$  |   $a_1$   |          |  $S_0$  |   $a_0$   |\n",
    "|  $S_1$  |   $a_0$   |          |  $S_1$  |   $a_0$   |\n",
    "|  $S_2$  |   $a_1$   |          |  $S_2$  |   $a_1$   |\n",
    "\n",
    "\n",
    "* <font color=\"green\">Definition: Given states, a distribution over actions\n",
    "$$ \\pi(a | s) = P [ A_t = a | S_t = s ] $$ </font>\n",
    "\n",
    "* Given a policy $\\pi$, MDP $M = ( S, A, P, R, \\gamma )$ is\n",
    "    * A Markov Process $( S, P^\\pi )$: $S_1, S_2, S_3, ...$ \n",
    "    * A Markov Reward Process $ (S, P^\\pi,, R^\\pi, \\gamma) $: $S_1, R_1, S_2, R_2, S_3, ...$ <br/>\n",
    "      where \n",
    "         $$\n",
    "             P^\\pi_{s,s^\\prime} = \\sum_{a \\in A} \\pi(a | s) P^a_{ss^\\prime} \\\\\n",
    "             R^\\pi_{s} = \\sum_{a \\in A} \\pi(a | s) R^a_{s}\n",
    "         $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Function (Action-Value Funciton, Q)\n",
    "\n",
    "* The expected return with policy $\\pi$, starting from state $s$ and taking an action $a$\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    Q^\\pi(s, a) &= E_\\pi [ G_t | S_t = s, A_t = a ] \\\\\n",
    "                &= E_\\pi [ R_{t+1} + \\gamma Q^\\pi (S_{t+1}, A_{t+1}) | S_t = s, A_t = a ]\n",
    "\\end{align}    \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approaches for Learning a Policy\n",
    "\n",
    "* Value Iteration\n",
    "* Policy Iteration\n",
    "* Linear Programming\n",
    "* Temporal Difference Learning\n",
    "    * Q-learning\n",
    "    * SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bellman Equation\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    V^\\pi(s) &= \\sum_{a \\in A} \\pi(a | s) Q^\\pi(s, a) \\\\\n",
    "             &= \\sum_{a \\in A} \\pi(a | s) \\big[ R_{s}^a + \\gamma \\sum_{s^\\prime \\in S} P^a_{ss^\\prime} V^\\pi(s^\\prime)   \\big] \\\\         \n",
    "    \\\\\n",
    "    \\\\\n",
    "    Q^\\pi (S_i) &= R_{s}^a + \\gamma \\sum_{s^\\prime \\in S} P^a_{ss^\\prime} V^\\pi(s^\\prime) \\\\\n",
    "                &= R_{s}^a + \\gamma \\sum_{s^\\prime \\in S} P^a_{ss^\\prime} \\sum_{a^\\prime \\in A} \\pi(a^\\prime | s^\\prime) Q^\\pi(s^\\prime, a^\\prime)\n",
    "    \\\\\n",
    "\\end{align}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimal Value Functions\n",
    "\n",
    "* Optimal Value Functions\n",
    "$$ V^*(s) = \\max_\\pi V^\\pi (s) $$\n",
    "$$ Q^*(s, a) = \\max_\\pi Q^\\pi (s, a) $$\n",
    "\n",
    "* Optimal Policy $ \\pi^* $ is \n",
    "$$ V^{\\pi^*} (s) = V^*(s) $$\n",
    "$$ Q^{\\pi^*} (s, a) = Q^*(s, a) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bellman Optimality Equation\n",
    "$$\n",
    "\\begin{align}\n",
    "    V^*(s) &= \\max_a Q^*(s, a) \\\\\n",
    "             &= \\max_a \\Big[ R_{s}^a + \\gamma \\sum_{s^\\prime \\in S} P^a_{ss^\\prime} V^*(s^\\prime) \\Big] \\\\         \n",
    "    \\\\\n",
    "    \\\\\n",
    "    Q^*(s,a) &= R_{ss^\\prime}^a + \\gamma \\sum_{s^\\prime \\in S} P^a_{ss^\\prime} V^*(s^\\prime) \\\\\n",
    "                &= R_{s}^a + \\gamma \\sum_{s^\\prime \\in S} P^a_{ss^\\prime} \\max_a Q^*(s^\\prime, a^\\prime)\n",
    "    \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value Iteration\n",
    "\n",
    "* For state transition from $i$ to $j$ with action $k$\n",
    "$$ V^{n+1} (s_i) = \\max_k \\Big[ R_i + \\gamma \\sum_{j=1}^N P^k_{ij} V^n (s_j) \\Big] $$\n",
    "\n",
    "* Value Iteration:\n",
    "<font size=3>\n",
    "$$ \n",
    "\\begin{align}\n",
    "n=0 \\quad \\forall i, \\quad &V^1 (s_i) = \\max_k \\Big[ R_i + \\gamma \\sum_{j=1}^N P^k_{ij} V^0 (s_j) \\Big] \\\\\n",
    "n=1 \\quad \\forall i, \\quad &V^2 (s_i) = \\max_k \\Big[ R_i + \\gamma \\sum_{j=1}^N P^k_{ij} V^1 (s_j) \\Big] \\\\\n",
    "n=2 \\quad \\forall i, \\quad &V^3 (s_i) = \\max_k \\Big[ R_i + \\gamma \\sum_{j=1}^N P^k_{ij} V^2 (s_j) \\Big] \\\\\n",
    "\\vdots\n",
    "\\end{align}\n",
    "$$\n",
    "</font>\n",
    "\n",
    "    * Convergence test: $ \\max_i \\Big| V^{n+1}(s_i) - V^n(s_i) \\Big| \\lt \\epsilon $. \n",
    "    \n",
    "    * Computing values for all $i$: Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example Value Function for MDP\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/21/Markov_Decision_Process_example.png\"\n",
    "width=350 />  \n",
    "\n",
    "* Let $\\gamma = 1$.\n",
    "\n",
    "| t | $V^\\pi_{S_0}$ | $V^\\pi_{S_1}$ | $V^\\pi_{S_2}$ |\n",
    "|:--|:-------------|:-------------|:-------------|\n",
    "| 0 |      0       |       0.0      |      0       |\n",
    "| 1 |      0       |       3.5      |    0.75      |\n",
    "| 2 |      0.75       |      4.525      |    1.5825      |\n",
    "| 3 |      1.5825       |      5.37675      |    2.420775      |\n",
    "| 4 |      2.420775     |      6.2163725   |    3.25945425      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Optimal Policy\n",
    "\n",
    "* Now, the best action in state $s_i$ as\n",
    "$$ \n",
    "    a^* = \\arg \\max_k \\Big[ R_i^k + \\gamma \\sum_j P_{ij}^k V^*(s_j) \\Big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Iteration\n",
    "\n",
    "![](http://webdocs.cs.ualberta.ca/~sutton/book/4/img158.gif)\n",
    "\n",
    "* Start with an initial policy $\\pi^0$.\n",
    "* Iteratively,\n",
    "    * Evaluate policy ($V^n(x) = V^{\\pi^n}(x)$):\n",
    "    $$ V^n(s) = R(s, a = \\pi^n(s)) + \\gamma \\sum_{s^\\prime} P(s^\\prime | s, a = \\pi^n(s)) V^{n}(s^\\prime) $$\n",
    "    * Improve policy:\n",
    "    $$ \\pi_{n+1}(s) = \\arg \\max_a \\Big[ R(s, a) + \\gamma \\sum_{s^\\prime} P(s^\\prime | s, a) V^{n}(s^\\prime) \\Big] $$\n",
    "    \n",
    "* Stop condition:\n",
    "    * Policy does not change anymore (for about 10 iterations)\n",
    "    * The changes in evaluation of values are minor \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Now, What are the problems with current approaches?\n",
    "\n",
    "* Large number of states\n",
    "    * explosive memory requirement!\n",
    "    \n",
    "    \n",
    "\n",
    "* Can we approximate the value function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Two Approaches\n",
    "* Model-based \n",
    "    - During exploration, learning a model ($P$ and $R$).\n",
    "    - Plan a policy based on the learned model (MDP)\n",
    "    - Strong theoretical results.\n",
    "    - Works well within manageable state space\n",
    "    \n",
    "\n",
    "* Model-free\n",
    "    - Do NOT learn a model\n",
    "    - Weaker theoretical results\n",
    "    - Often works well with large state space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Temporal Difference (TD) Learning\n",
    "\n",
    "* Model-free Learning\n",
    "* Learning from incomplete episodes\n",
    "* Bootstrapping: starting from an estimation until it gets closer to true value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TD(0)\n",
    "\n",
    "* Temporal Difference Error \n",
    "    - with an estimation of $\\hat{V}(s_{t+1})$,\n",
    "    $$ \n",
    "        \\begin{align}\n",
    "        V(s_t) &= R_{t+1} + \\gamma V(s_{t+1}) \\\\\n",
    "        V(s_t) &\\sim R_{t+1} + \\gamma \\hat{V}(s_{t+1}) \\\\\n",
    "        \\Rightarrow \\quad \\delta_t &= R_{t+1} + \\gamma \\hat{V}(s_{t+1}) - V(s_t)\n",
    "        \\end{align}\n",
    "    $$\n",
    "* Gradient (Bootstrapping) Update for $V$ with learning rate $\\alpha$:\n",
    "$$\n",
    "    \\begin{align}\n",
    "        V(s_t) &\\leftarrow R_{t+1} + \\alpha \\delta_t \\\\\n",
    "        V(s_t) &\\leftarrow R_{t+1} + \\alpha (R_{t+1} + \\gamma \\hat{V}(s_{t+1}) - V(s_t))\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Control Problems\n",
    "\n",
    "* need to consider both state and action \n",
    "* Q value \n",
    "* On-policy\n",
    "    - \"_Learn on the Job_\"\n",
    "    - Learn about policy $\\pi$ from experience sampled from $\\pi$\n",
    "* Off-policy\n",
    "    - \"_Look over someone's shoulder_\"\n",
    "    - Learn about policy $\\pi$ from experience samples from other policies (non $\\pi$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Action Selection\n",
    "\n",
    "- Greedy \n",
    "\n",
    "$$\n",
    "a^* = \\arg \\max_a Q(S_t, a)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "- exploration-exploitation dilemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "- $\\epsilon$-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SARSA \n",
    "\n",
    "![](http://webdocs.cs.ualberta.ca/~sutton/book/ebook/imgtmp8.png)\n",
    "\n",
    "* State-Action-Reward-State-Action\n",
    "* On-policy Control\n",
    "\n",
    "$$\n",
    "    Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha ( R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)) \n",
    "$$\n",
    "\n",
    "![](http://webdocs.cs.ualberta.ca/~sutton/book/ebook/pseudotmp8.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementation in the note\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-Learning\n",
    "\n",
    "* Off-policy Control\n",
    "\n",
    "\n",
    "$$\n",
    "    Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha ( R_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)) \n",
    "$$\n",
    "\n",
    "![](http://webdocs.cs.ualberta.ca/~sutton/book/ebook/pseudotmp9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NEXT: Unsupervised Learning"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
