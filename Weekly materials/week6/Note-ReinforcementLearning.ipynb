{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    " \\newcommand{\\wv}{\\mathbf{w}}\n",
    " \\newcommand{\\yv}{\\mathbf{y}}\n",
    " \\newcommand{\\zv}{\\mathbf{z}}\n",
    " \\newcommand{\\Chi}{\\mathcal{X}}\n",
    " \\newcommand{\\R}{\\rm I\\!R}\n",
    " \\newcommand{\\sign}{\\text{sign}}\n",
    " \\newcommand{\\Tm}{\\mathbf{T}}\n",
    " \\newcommand{\\Xm}{\\mathbf{X}}\n",
    " \\newcommand{\\Zm}{\\mathbf{Z}}\n",
    " \\newcommand{\\I}{\\mathbf{I}}\n",
    " \\newcommand{\\muv}{\\boldsymbol\\mu}\n",
    " \\newcommand{\\Sigmav}{\\boldsymbol\\Sigma}\n",
    "$\n",
    "### ITCS6155\n",
    "\n",
    "\n",
    "# Reinforcement Learning\n",
    "\n",
    "Along with supervised and unsupervised learning, reinforcement learning is one of the interesting fields of machine learining. \n",
    "As we shortly discussed in the first week of class, reinforcement learning is different from other machine learning paradigms as summarized below: \n",
    "\n",
    "- only reward signal as feedback,\n",
    "- the feedback can be delayed, \n",
    "- sequential data, \n",
    "- interaction based on the actions taken. \n",
    "\n",
    "The reinforcement learning resembles huuman learning or animal training that treats reward good behavior. \n",
    "When the series of actions end up with good results, we can **reinforce** those actions by giving some rewards.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"http://webpages.uncc.edu/mlee173/teach/itcs6156/images/class/werber_ls.jpeg\" height=300/>\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"http://webpages.uncc.edu/mlee173/teach/itcs6156/images/class/pavlovs-dogs-mark-stivers.gif\" width=400/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "## Applictions\n",
    "\n",
    "\n",
    "There are many possible applications: \n",
    "\n",
    "- walking robot, \n",
    "- playing boardgames, \n",
    "- controlling joysticks for video games,\n",
    "- smart thermostat,\n",
    "- stock trading,\n",
    "- music personalization,\n",
    "- recommendation system,\n",
    "- marketing,\n",
    "- product delivery,\n",
    "- and so on...\n",
    "\n",
    "<img src=\"http://webpages.uncc.edu/mlee173/teach/itcs6156/images/class/RLexamples.png\" width=600/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Terminology\n",
    "\n",
    "- Reward: $R_t$ is a scalar feedback signal at time $t$. It indicates how well agent is doing at the time. \n",
    "\n",
    "- State: $S_t$ represents what happens currently and next. \n",
    "\n",
    "- Action: $A_t$ is how an agent affects to the environment that can change the state. \n",
    "\n",
    "- Observation: $O_t$ is what an agent recognizes the world for the state $S_t$. \n",
    "\n",
    "- Policy: A function that determines an agent's behavior or a function that selects its action, $\\pi(S_t)$. \n",
    "\n",
    "Q: Think about the rewards for the example applications above, and answer the possible rewards for them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <font color=\"red\">Answer HERE </font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling \n",
    "\n",
    "How can we simplify the listed problems to solve? We have the sets of decision making problems or control problems.\n",
    "With the assumption of **Markov property**, we can model the problems as Makrov Decision Process (MDP).\n",
    "\n",
    "Here, the definition of Markov property is as follows,\n",
    "\n",
    "**Definition**: A state $S_t$ is Markov if and only if \n",
    "<br/><br/>\n",
    "$$P [ S_{t+1} | S_t ] = P[ S_{t+1} | S_1, ..., S_t ].$$\n",
    "\n",
    "That is, we can say the state transition model has Markov property when the future is dependent only on the current state not the past. \n",
    "\n",
    "\n",
    "### Markov Decision Processes\n",
    "So, the MDP can be defined as follows and the example state transition diagram has the transition probabilities and rewards along with discounting factor. The discounting factor ensures the mathematical simplicity, and it allows the modeling the uncertainty of the future as well. \n",
    "\n",
    " <font color='red' size=5>$ ( S, A, P, R, \\gamma ) $ </font>\n",
    "* $S$ : a finite set of states\n",
    "* $A$ : a finite set of actions\n",
    "* $P$ : a state transition probability\n",
    "* $P^a_{ss^\\prime} = P [ S_{t+1} = s^\\prime | S_t = s, A_t = a ]$\n",
    "* $R$ : a reward function\n",
    "* $\\gamma$ : a discount factor\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/21/Markov_Decision_Process_example.png\"\n",
    "width=500 />  \n",
    "<center> MDP with 3 states and 2 actions (wikipedia) </center>\n",
    "\n",
    "\n",
    "### Policy\n",
    "\n",
    "The policy $\\pi(S_t)$ is a function that maps from a state to an action. Thus, it can be easily represented by a probability: \n",
    "\n",
    "$$ \\pi(a | s) = P[A_t = a | S_t = s]. $$\n",
    "\n",
    "Or, it can be a machine learning model to generate such probability for a certain behavior of an agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Now with the MDP model, what we want to do is find a policy $\\pi$ that maximizes the long-term rewards. The long-term rewards can be modeled as the sum of rewards. Here, considering the different importance on the actions (the current reward is not equivalent to future rewards), we can define the **Return**, $G_t$, \n",
    "\n",
    "$$ G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}. $$\n",
    "\n",
    "From this, we can model the long-term value of a certain state $s$ with the return:\n",
    "\n",
    "$$ V(s) = E [ G_t | S_t = s ]. $$\n",
    "\n",
    "We, we set our objective to maximize the return. Developing the optimal policy can be made by defining the evaluation function of state and action pair, the Q funciton: \n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    Q^\\pi(s, a) &= E_\\pi [ G_t | S_t = s, A_t = a ] \\\\\n",
    "                &= E_\\pi [ R_{t+1} + \\gamma Q^\\pi (S_{t+1}, A_{t+1}) | S_t = s, A_t = a ]\n",
    "\\end{align}    \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to reach the goal? / How to find the optimal policy?\n",
    "\n",
    "There are several ways to reach the goal such as value iteration, policy iteration, linear programming, and temporal difference learning. \n",
    "\n",
    "\n",
    "First, let us examine the bootstraping representation of the value function to solve the problem.\n",
    "This is called *Bellman equation*. \n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "  V(s) &= E [ G_t | S_t = s ] \\\\\n",
    "       &= E [ R_{t+1} + \\gamma R_{t+2} + \\cdots | S_t = s ] \\\\\n",
    "       &= E [ R_{t+1} + \\gamma ( R_{t+2} + \\gamma R_{t+3} + \\cdots ) | S_t = s ] \\\\\n",
    "       &= E [ R_{t+1} + \\gamma G_{t+1} | S_t = s ] \\\\\n",
    "       &= E [ R_{t+1} + \\gamma V(s_{t+1}) | S_t= s ]\\\\\n",
    "      \\\\\n",
    "  V(s) &= R(s) + \\gamma \\sum_{s^\\prime \\in S} P_{ss^\\prime} V(s^\\prime)      \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "With matrix representation, we can rewrite this as follows:\n",
    "\n",
    "$$\n",
    " \\begin{align}\n",
    "  \\begin{pmatrix}\n",
    "  V_1 \\\\\n",
    "  V_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  V_n  \n",
    " \\end{pmatrix} &= \n",
    "  \\begin{pmatrix}\n",
    "  R_1 \\\\\n",
    "  R_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  R_n  \n",
    " \\end{pmatrix} +   \n",
    " \\gamma\n",
    " \\begin{pmatrix}\n",
    "  P_{11} & P_{12} & \\cdots & P_{1n} \\\\\n",
    "  P_{21} & P_{22} & \\cdots & P_{2n} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  P_{n1} & P_{n2} & \\cdots & P_{nn} \n",
    " \\end{pmatrix}\n",
    "  \\begin{pmatrix}\n",
    "  V_1 \\\\\n",
    "  V_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  V_n  \n",
    " \\end{pmatrix}\n",
    " \\\\\n",
    " \\\\\n",
    " V &= R + \\gamma PV \n",
    " \\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "This can be directly solved with\n",
    "\n",
    "$$ V = (I - \\gamma P)^{-1} R. $$\n",
    "\n",
    "Considering the computational complexity with inverse matrix, however, we can consider iterative solutions such as dynamic programming, Monte-Carlo evaluation, or temporal difference learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "* For state transition from $i$ to $j$ with action $k$\n",
    "$$ V^{n+1} (s_i) = \\max_k \\Big[ R_i + \\gamma \\sum_{j=1}^N P^k_{ij} V^n (s_j) \\Big] $$\n",
    "\n",
    "* Value Iteration:\n",
    "<font size=3>\n",
    "$$ \n",
    "\\begin{align}\n",
    "n=0 \\quad \\forall i, \\quad &V^1 (s_i) = \\max_k \\Big[ R_i + \\gamma \\sum_{j=1}^N P^k_{ij} V^0 (s_j) \\Big] \\\\\n",
    "n=1 \\quad \\forall i, \\quad &V^2 (s_i) = \\max_k \\Big[ R_i + \\gamma \\sum_{j=1}^N P^k_{ij} V^1 (s_j) \\Big] \\\\\n",
    "n=2 \\quad \\forall i, \\quad &V^3 (s_i) = \\max_k \\Big[ R_i + \\gamma \\sum_{j=1}^N P^k_{ij} V^2 (s_j) \\Big] \\\\\n",
    "\\vdots\n",
    "\\end{align}\n",
    "$$\n",
    "</font>\n",
    "\n",
    "* Convergence can be tested as follows: \n",
    "\n",
    "$$ \\max_i \\Big| V^{n+1}(s_i) - V^n(s_i) \\Big| \\lt \\epsilon. $$ \n",
    "    \n",
    "* Computing values for all $i$'s with Dynamic Programming. \n",
    "\n",
    "\n",
    "Here follows an example from Wikipedia. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/21/Markov_Decision_Process_example.png\"\n",
    "width=350 />  \n",
    "\n",
    "Let $\\gamma = 1$. The values for each state can be iteratively updated as the following table. \n",
    "\n",
    "| t | $V^\\pi_{S_0}$ | $V^\\pi_{S_1}$ | $V^\\pi_{S_2}$ |\n",
    "|:--|:-------------|:-------------|:-------------|\n",
    "| 0 |      0       |       0.0      |      0       |\n",
    "| 1 |      0       |       3.5      |    0.75      |\n",
    "| 2 |      0.75       |      4.525      |    1.5825      |\n",
    "| 3 |      1.5825       |      5.37675      |    2.420775      |\n",
    "| 4 |      2.420775     |      6.2163725   |    3.25945425      |\n",
    "\n",
    "In Python, we can write the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 1.0\n",
    "n_states = 3\n",
    "n_actions = 2\n",
    "\n",
    "# value vector\n",
    "V = np.zeros(n_states).reshape((-1, 1))\n",
    "# Reward dictionary\n",
    "R = {(2,0): -1, (1,0): 5}\n",
    "# transition probability: key (action, from, to)\n",
    "P = {(0,0,0): 0.5, (0,0,2): 0.5, (1,0,2): 1.0, \n",
    "     (0,1,0): 0.7, (0,1,1): 0.1, (0,1,2): 0.2,\n",
    "     (1,1,1): 0.95, (1,1,2): 0.05,\n",
    "     (0,2,0): 0.4, (0,2,2): 0.6, \n",
    "     (1,2,0): 0.3, (1,2,1): 0.3, (1,2,2): 0.4 }\n",
    "\n",
    "def Rf(s, sn):\n",
    "    if (s, sn) in R:\n",
    "        return R[(s,sn)]\n",
    "    return 0\n",
    "\n",
    "def Pf(s, sn, a):\n",
    "    if (a, s, sn) in P:\n",
    "        return P[(a,s,sn)]\n",
    "    return 0.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" with no matrix\"\"\"\n",
    "def Rs(s, a):\n",
    "    r = 0\n",
    "    for j in range(3):\n",
    "        if (a, s, j) in P:\n",
    "            r += P[(a, s, j)] * Rf(s, j)\n",
    "    return r\n",
    "\n",
    "\n",
    "def Pfv(s, a):\n",
    "    ret = np.zeros(3).reshape((-1, 1))\n",
    "    for j in range(3):\n",
    "        if (a, s, j) in P:\n",
    "            ret[j] = P[(a, s, j)]\n",
    "    return ret\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t= 0 \tV =  [[ 0.  0.  0.]]\n",
      "t= 1 \tV =  [[ 0.   3.5 -0.3]]\n",
      "t= 2 \tV =  [[ 0.   7.  -0.6]]\n",
      "t= 3 \tV =  [[  0.   10.5  -0.9]]\n",
      "t= 4 \tV =  [[  0.   14.   -1.2]]\n",
      "t= 5 \tV =  [[  0.   17.5  -1.5]]\n"
     ]
    }
   ],
   "source": [
    "Rmat = np.zeros((n_states, n_actions))\n",
    "Pmat = np.zeros((n_actions, n_states, n_states))\n",
    "\n",
    "for i in range(n_states):\n",
    "    for k in range(n_actions):\n",
    "        for j in range(n_states):\n",
    "            if (k, i, j) in P:\n",
    "                Rmat[i, k] += P[(k, i, j)] * Rf(i, j)\n",
    "\n",
    "for k, i, j in P.keys():\n",
    "    Pmat[k,i,j] = P[(k, i, j)]\n",
    "\n",
    "\n",
    "for t in range(5):\n",
    "    print(\"t=\", t, \"\\tV = \", V.T)\n",
    "    # Bellman\n",
    "    newV = Rmat + gamma * np.sum(Pmat * V, axis=2).T\n",
    "    V[:] = np.max(newV, axis=1, keepdims=True)\n",
    "    \n",
    "print(\"t=\", t+1, \"\\tV = \", V.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t= 0 \tV =  [[  0.   17.5  -1.5]]\n",
      "t= 1 \tV =  [[ -0.75  16.55   3.84]]\n",
      "t= 2 \tV =  [[  3.84     15.9145    7.16235]]\n",
      "t= 3 \tV =  [[  7.16235     15.4768925    9.35671275]]\n",
      "t= 4 \tV =  [[  9.35671275  15.17088351  10.80096398]]\n",
      "t= 5 \tV =  [[ 10.80096398  14.95238754  11.74639105]]\n",
      "t= 6 \tV =  [[ 11.74639105  15.56699069  12.59257094]]\n",
      "t= 7 \tV =  [[ 12.59257094  16.39001292  13.43180353]]\n",
      "t= 8 \tV =  [[ 13.43180353  17.22762447  14.27054981]]\n",
      "t= 9 \tV =  [[ 14.27054981  18.06625728  15.10926205]]\n",
      "t= 10 \tV =  [[ 15.10926205  18.90496158  15.94797191]]\n",
      "t= 11 \tV =  [[ 15.94797191  19.74367088  16.7866816 ]]\n",
      "t= 12 \tV =  [[ 16.7866816   20.58238053  17.62539128]]\n",
      "t= 13 \tV =  [[ 17.62539128  21.4210902   18.46410096]]\n",
      "t= 14 \tV =  [[ 18.46410096  22.25979988  19.30281063]]\n",
      "t= 15 \tV =  [[ 19.30281063  23.09850956  20.14152031]]\n",
      "t= 16 \tV =  [[ 20.14152031  23.93721924  20.98022999]]\n",
      "t= 17 \tV =  [[ 20.98022999  24.77592891  21.81893967]]\n",
      "t= 18 \tV =  [[ 21.81893967  25.61463859  22.65764934]]\n",
      "t= 19 \tV =  [[ 22.65764934  26.45334827  23.49635902]]\n",
      "t= 20 \tV =  [[ 23.49635902  27.29205795  24.3350687 ]]\n",
      "t= 21 \tV =  [[ 24.3350687   28.13076762  25.17377838]]\n",
      "t= 22 \tV =  [[ 25.17377838  28.9694773   26.01248805]]\n",
      "t= 23 \tV =  [[ 26.01248805  29.80818698  26.85119773]]\n",
      "t= 24 \tV =  [[ 26.85119773  30.64689666  27.68990741]]\n",
      "t= 25 \tV =  [[ 27.68990741  31.48560633  28.52861709]]\n",
      "t= 26 \tV =  [[ 28.52861709  32.32431601  29.36732676]]\n",
      "t= 27 \tV =  [[ 29.36732676  33.16302569  30.20603644]]\n",
      "t= 28 \tV =  [[ 30.20603644  34.00173537  31.04474612]]\n",
      "t= 29 \tV =  [[ 31.04474612  34.84044504  31.8834558 ]]\n",
      "t= 30 \tV =  [[ 31.8834558   35.67915472  32.72216547]]\n",
      "t= 31 \tV =  [[ 32.72216547  36.5178644   33.56087515]]\n",
      "t= 32 \tV =  [[ 33.56087515  37.35657408  34.39958483]]\n",
      "t= 33 \tV =  [[ 34.39958483  38.19528375  35.23829451]]\n",
      "t= 34 \tV =  [[ 35.23829451  39.03399343  36.07700418]]\n",
      "t= 35 \tV =  [[ 36.07700418  39.87270311  36.91571386]]\n",
      "t= 36 \tV =  [[ 36.91571386  40.71141278  37.75442354]]\n",
      "t= 37 \tV =  [[ 37.75442354  41.55012246  38.59313321]]\n",
      "t= 38 \tV =  [[ 38.59313321  42.38883214  39.43184289]]\n",
      "t= 39 \tV =  [[ 39.43184289  43.22754182  40.27055257]]\n",
      "t= 40 \tV =  [[ 40.27055257  44.06625149  41.10926225]]\n",
      "t= 41 \tV =  [[ 41.10926225  44.90496117  41.94797192]]\n",
      "t= 42 \tV =  [[ 41.94797192  45.74367085  42.7866816 ]]\n",
      "t= 43 \tV =  [[ 42.7866816   46.58238053  43.62539128]]\n",
      "t= 44 \tV =  [[ 43.62539128  47.4210902   44.46410096]]\n",
      "t= 45 \tV =  [[ 44.46410096  48.25979988  45.30281063]]\n",
      "t= 46 \tV =  [[ 45.30281063  49.09850956  46.14152031]]\n",
      "t= 47 \tV =  [[ 46.14152031  49.93721924  46.98022999]]\n",
      "t= 48 \tV =  [[ 46.98022999  50.77592891  47.81893967]]\n",
      "t= 49 \tV =  [[ 47.81893967  51.61463859  48.65764934]]\n",
      "t= 50 \tV =  [[ 48.65764934  52.45334827  49.49635902]]\n"
     ]
    }
   ],
   "source": [
    "gamma=1\n",
    "for t in range(50):\n",
    "    print(\"t=\", t, \"\\tV = \", V.T)\n",
    "    for i in range(3):\n",
    "        # Bellman\n",
    "        V[i] = max(Rs(i, 0) + gamma * np.sum(Pfv(i, 0) * V), \n",
    "                   Rs(i, 1) + gamma * np.sum(Pfv(i, 1) * V))\n",
    "print(\"t=\", t+1, \"\\tV = \", V.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "![](http://webdocs.cs.ualberta.ca/~sutton/book/4/img158.gif)\n",
    "\n",
    "* Start with an initial policy $\\pi^0$.\n",
    "* Iteratively,\n",
    "    * Evaluate policy ($V^n(x) = V^{\\pi^n}(x)$):\n",
    "    $$ V^n(s) = R(s, a = \\pi^n(s)) + \\gamma \\sum_{s^\\prime} P(s^\\prime | s, a = \\pi^n(s)) V^{n}(s^\\prime) $$\n",
    "    * Improve policy:\n",
    "    $$ \\pi_{n+1}(s) = \\arg \\max_a \\Big[ R(s, a) + \\gamma \\sum_{s^\\prime} P(s^\\prime | s, a) V^{n}(s^\\prime) \\Big] $$\n",
    "    \n",
    "* Stop condition:\n",
    "    * Policy does not change anymore (for about 10 iterations)\n",
    "    * The changes in evaluation of values are minor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  0 \t [0 0 1]\t [ 0.    3.5   0.75]\n",
      "n =  1 \t [1 0 1]\t [ 0.75    4.525   1.5825]\n",
      "n =  2 \t [1 0 1]\t [ 1.5825    5.37675   2.420775]\n",
      "n =  3 \t [1 0 1]\t [ 2.420775    6.2163725   3.25945425]\n",
      "n =  4 \t [1 0 1]\t [ 3.25945425  7.05514607  4.0981618 ]\n",
      "n =  5 \t [1 0 1]\t [ 4.0981618   7.89386023  4.93687133]\n",
      "n =  6 \t [1 0 1]\t [ 4.93687133  8.73257022  5.77558099]\n",
      "n =  7 \t [1 0 1]\t [ 5.77558099  9.57127992  6.61429067]\n",
      "n =  8 \t [1 0 1]\t [  6.61429067  10.40998959   7.45300035]\n",
      "n =  9 \t [1 0 1]\t [  7.45300035  11.24869927   8.29171002]\n"
     ]
    }
   ],
   "source": [
    "n_iter = 10\n",
    "gamma = 1.\n",
    "V = np.zeros(3)\n",
    "\n",
    "pi = np.random.randint(2, size=n_states)\n",
    "\n",
    "for n in range(n_iter):\n",
    "    print(\"n = \", n, \"\\t\", pi, end=\"\")\n",
    "    \n",
    "    # evaluate\n",
    "    for s in range(n_states):\n",
    "        V[s] = Rs(s, pi[s]) + gamma * np.sum([Pf(s, sn, pi[s]) * V[sn] for sn in range(n_states)])\n",
    "    print(\"\\t\", V)\n",
    "    \n",
    "    # improve\n",
    "    for s in range(n_states):\n",
    "        pi[s] = np.argmax([Rs(s, a) + gamma * np.sum([Pf(s, sn, a) * V[sn] for sn in range(n_states)])  for a in range(n_actions)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Programming\n",
    "\n",
    "                                                                                    [Manne '60]\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{x}{\\text{minimize}}\n",
    "& & \\sum_x V(x) \\\\\n",
    "& \\text{subject to}\n",
    "& & V(x) \\geq R(x, u) + \\gamma \\sum_{x^\\prime} P(x^\\prime | x, u) V^{n}(x^\\prime) &\\forall x, a.\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning\n",
    "\n",
    "When there are large number of states, the memory requirement grows exponentially. \n",
    "*Temporal difference (TD) learning* considers that the agent knows only the partial information of the MDP. \n",
    "With only current and next state transition and without any model transition probability, TD lets the agent explore the environment to examine the random policy. \n",
    "With an estiate of the value function $V(s)$, $\\hat{V}(s)$, \n",
    "\n",
    "   $$ \n",
    "        \\begin{align}\n",
    "        V(s_t) &= R_{t+1} + \\gamma V(s_{t+1}) \\\\\n",
    "        V(s_t) &\\sim R_{t+1} + \\gamma \\hat{V}(s_{t+1}) \\\\\n",
    "        \\Rightarrow \\quad \\delta_t &= R_{t+1} + \\gamma \\hat{V}(s_{t+1}) - V(s_t).\n",
    "        \\end{align}\n",
    "    $$\n",
    "    \n",
    "Here, $\\delta$ represents the *temporal diffrence error*. \n",
    "We can use this error as a gradient to update the value estimation.\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        V(s_t) &\\leftarrow R_{t+1} + \\alpha \\delta_t \\\\\n",
    "        V(s_t) &\\leftarrow R_{t+1} + \\alpha (R_{t+1} + \\gamma \\hat{V}(s_{t+1}) - V(s_t))\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "## Example Model\n",
    "\n",
    "<img src=\"http://webpages.uncc.edu/mlee173/teach/itcs6156/images/class/rl_simple_model.png\" width=700 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model (problem to solve)\n",
    "# a: 0 - loop  1 - to next state\n",
    "# r:       +1                 +0      +100 when reaches 4\n",
    "#\n",
    "n_states = 5\n",
    "n_actions = 2\n",
    "\n",
    "def transition(s, a):\n",
    "    s1 = s + a\n",
    "    if s == s1: \n",
    "        r = 1\n",
    "    elif s1 == n_states-1:\n",
    "        r = 100\n",
    "    else:\n",
    "        r = 0\n",
    "    return s1, r\n",
    "\n",
    "def pause_print():\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(\"\\033[F\")\n",
    "    sys.stdout.flush()\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: [0 0 0 1 0]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [ 6.51  0.    0.    0.    0.  ]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [ 8.78  0.    0.    0.    0.  ]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [ 9.58  0.    0.    0.    0.  ]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [ 9.85  0.    0.    0.    0.  ]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [ 9.95  0.    0.    0.    0.  ]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [ 9.98  0.    0.    0.    0.  ]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [ 9.99  0.    0.    0.    0.  ]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [10.  0.  0.  0.  0.]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [10.  0.  0.  0.  0.]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [10.  0.  0.  0.  0.]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [10.  0.  0.  0.  0.]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [10.  0.  0.  0.  0.]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [10.  0.  0.  0.  0.]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [10.  0.  0.  0.  0.]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [10.  0.  0.  0.  0.]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [10.  0.  0.  0.  0.]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [10.  0.  0.  0.  0.]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [10.  0.  0.  0.  0.]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [10.  0.  0.  0.  0.]\n",
      "\tTraj:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \t [10.  0.  0.  0.  0.]\n",
      "\n",
      "Policy: [1 1 1 1 1]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [   0.    0.    0.  100.    0.]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [   0.    0.   90.  100.    0.]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [   0.   81.   90.  100.    0.]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\tTraj:  0 1 1 1 2 1 3 1 4 \t [  72.9   81.    90.   100.     0. ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example TD(0) Update\n",
    "alpha = 1.0\n",
    "gamma = 0.9\n",
    "V = np.zeros(n_states)  # 5 states\n",
    "pi = np.random.randint(n_actions, size=n_states)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "for k in range(2):\n",
    "    if k == 1:\n",
    "        pi = np.ones(n_states, dtype=np.int)\n",
    "    print(\"Policy:\", pi)\n",
    "    # Evaluation of the random policy\n",
    "    for e in range(20):\n",
    "        s = 0 \n",
    "\n",
    "        print(\"\\r\\tTraj: \", s, end=\" \")\n",
    "        for step in range(10):\n",
    "            a = pi[s]\n",
    "            s1, r1 = transition(s, a)\n",
    "            V[s] += alpha * (r1 + gamma * V[s1] - V[s])\n",
    "            s = s1\n",
    "\n",
    "            print(a, s, end=\" \")\n",
    "            if s == n_states-1:\n",
    "                break\n",
    "\n",
    "        print(\"\\t\", V, end=\"\\n\")\n",
    "        #pause_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control Problems and Q function\n",
    "\n",
    "For control problems, we have defined Q function above to evaluate the state and action altogether.\n",
    "Updating the Q values with TD learning is similar to previous update with two different considerations. \n",
    "First, we update the Q with assumption that we follow a certain behavior policy. Thus, we call this as *on-policy control*, or **SARSA**. \n",
    "\n",
    "$$\n",
    "    Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha ( R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)) \n",
    "$$\n",
    "\n",
    "Next, without making assumption of behavior policy, we can explore other possible policies to update the Q. We call this as *off-policy control*, or **Q-learning**. \n",
    "\n",
    "\n",
    "$$\n",
    "    Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha ( R_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)) \n",
    "$$\n",
    "\n",
    "Here describes the psedocode for each algorithm. \n",
    "\n",
    "\n",
    "**[Algorithm: TD Learning]**\n",
    "![](http://incompleteideas.net/book/ebook/pseudotmp7.png)\n",
    "\n",
    "**[Algorithm: SARSA]**\n",
    "![](http://incompleteideas.net/book/ebook/pseudotmp8.png)\n",
    "\n",
    "\n",
    "**[Algorithm: Q-learning]**\n",
    "![](http://incompleteideas.net/book/ebook/pseudotmp9.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing an Action\n",
    "\n",
    "Picking an action can be simple by selecting one with maximum Q value, so *Greedy!*. \n",
    "\n",
    "$$\n",
    "a^* = \\arg \\max_a Q(S_t, a)\n",
    "$$\n",
    "\n",
    "However, this can cause limited experience to develop good Q estimation, and eventually a good policy. \n",
    "Without new data, greedy action selection will repeat the same actions, or repeatedly *exploit* your current knowledge. Thus, you need to *explore* other non-greedy actions to increase the experience to improve the Q estimation. \n",
    "\n",
    "This is called \"exploration-exploitation dilemma.\" \n",
    "\n",
    "\n",
    "One of the way for this dilemma is $\\epsilon$-greedy action selection. With a parameter $\\epsilon \\in [0, 1]$, we can control the exploration and exploitation level. When $\\epsilon = 0$, the actions are selected in greedy manner, but when $\\epsilon = 1$, the actions are selected randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# action selection\n",
    "def greedy(Q, s):\n",
    "    return np.argmax(Q[s])  # greedy action selection\n",
    "\n",
    "def e_greedy(Q, s, e):   \n",
    "    if np.random.rand() < e:\n",
    "        return np.random.randint(n_actions)\n",
    "    else:\n",
    "        return greedy(Q,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q:  [[   2.42   73.  ]\n",
      " [  74.     81.12]\n",
      " [  82.12   90.13]\n",
      " [  91.13  100.14]\n",
      " [   0.33    0.16]]\n",
      "Policy: [1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# SARSA example\n",
    "alpha = 1.  # learning rate\n",
    "gamma = 0.9 # discount factor\n",
    "epsilon = 0.1\n",
    "\n",
    "# tabular approximation\n",
    "Q = np.random.rand(n_states, n_actions)  # 5 states and 2 actions\n",
    "\n",
    "for e in range(20):\n",
    "    s = 0 \n",
    "    a = e_greedy(Q, s, epsilon)  # greedy action selection\n",
    "    \n",
    "    for step in range(100):\n",
    "        s1, r1 = transition(s, a)\n",
    "        a1 = e_greedy(Q, s, epsilon)\n",
    "        \n",
    "        # TODO: update Q table here!\n",
    "        \n",
    "        \n",
    "        #print(\"s: \", s, \"a: \", a, \"s1:\", s1, \"a1:\", a1, \"Q: \", Q[s,a])\n",
    "        s, a = s1, a1\n",
    "        if s == n_states-1:\n",
    "            break\n",
    "    \n",
    "print(\"Final Q: \", Q)\n",
    "print(\"Policy:\", np.argmax(Q, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:  0 a:  s1: 0 a1: 1 Q:  1.24006438385\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  2.11605794546\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  2.90445215091\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  3.61400693582\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  4.25260624224\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  4.82734561802\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  0.215898229533\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  0.317105678191\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  1.31710567819\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  2.18539511037\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  2.96685559933\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  3.6701700394\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  4.30315303546\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  4.87283773191\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  5.38555395872\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  5.84699856285\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  6.26229870657\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  6.63606883591\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  6.97246195232\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  7.27521575709\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  7.54769418138\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  7.79292476324\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  8.01363228692\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  8.21226905822\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  8.3910421524\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  8.55193793716\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  8.69674414345\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  8.8270697291\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  8.94436275619\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.04992648057\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.14493383251\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.23044044926\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.30739640434\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.3766567639\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.43899108751\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.49509197876\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.54558278089\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.5910245028\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.63192205252\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.66872984727\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.70185686254\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.73167117629\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.75850405866\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.78265365279\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  0.477446642928\n",
      "s:  3 a:  s1: 3 a1: 1 Q:  1.47744664293\n",
      "s:  3 a:  s1: 3 a1: 1 Q:  2.32970197863\n",
      "s:  3 a:  s1: 3 a1: 1 Q:  3.09673178077\n",
      "s:  3 a:  s1: 3 a1: 1 Q:  3.78705860269\n",
      "s:  3 a:  s1: 3 a1: 1 Q:  4.40835274242\n",
      "s:  3 a:  s1: 3 a1: 1 Q:  4.96751746818\n",
      "s:  3 a:  s1: 3 a1: 1 Q:  5.47076572136\n",
      "s:  3 a:  s1: 3 a1: 1 Q:  5.92368914923\n",
      "s:  3 a:  s1: 3 a1: 1 Q:  6.3313202343\n",
      "s:  3 a:  s1: 3 a1: 1 Q:  6.69818821087\n",
      "s:  3 a:  s1: 3 a1: 1 Q:  7.02836938979\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  5.34461105622\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  5.81014995059\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  6.22913495553\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  6.60622145998\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  0.285395110372\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  8.80438828751\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.80438828751\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.82394945876\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.84155451288\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.8573990616\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.87165915544\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.88449323989\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.8960439159\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.90643952431\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.91579557188\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.92421601469\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.93179441322\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.9386149719\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.94475347471\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.95027812724\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.95525031452\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.95972528306\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.96375275476\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.96737747928\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.97063973135\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.97357575822\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.9762181824\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.97859636416\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.98073672774\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.98266305497\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.98439674947\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.98595707452\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.98736136707\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.98862523036\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.98976270733\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99078643659\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99170779294\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99253701364\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99328331228\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99395498105\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99455948294\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99510353465\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99559318119\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99603386307\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99643047676\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99678742908\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99710868618\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99739781756\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.9976580358\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99789223222\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.998103009\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.9982927081\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99846343729\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99861709356\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.9987553842\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99887984578\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99899186121\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99909267509\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99918340758\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99926506682\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99933856014\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99940470412\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99946423371\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99951781034\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99956602931\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99960942638\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99964848374\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99968363536\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99971527183\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99974374464\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99976937018\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99979243316\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99981318985\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99983187086\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99984868378\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.9998638154\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99987743386\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99988969047\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99990072143\n",
      "s:  2 a:  s1: 2 a1: 1 Q:  9.99991064928\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  6.94559931398\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  7.25103938258\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  7.52593544433\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  7.77334189989\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  7.9960077099\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  8.19640693891\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  8.37676624502\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  8.53908962052\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  7.92394945876\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  8.68518065847\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  8.81666259262\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  8.93499633336\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  9.04149670002\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  9.13734703002\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  9.22361232702\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  9.30125109432\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  9.37112598489\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  9.4340133864\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  9.49061204776\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  9.54155084298\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  9.58739575868\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  9.62865618281\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  9.66579056453\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  9.69921150808\n",
      "s:  0 a:  s1: 0 a1: 1 Q:  9.72929035727\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 1 a1: 1 Q:  74.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 3 a1: 1 Q:  91.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 1 a1: 1 Q:  74.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "s:  0 a:  s1: 1 a1: 1 Q:  73.0558878812\n",
      "s:  1 a:  s1: 2 a1: 1 Q:  81.1732087569\n",
      "s:  2 a:  s1: 3 a1: 1 Q:  90.1924541743\n",
      "s:  3 a:  s1: 4 a1: 1 Q:  100.213837971\n",
      "Final Q:  [[  9.73e+00   7.31e+01]\n",
      " [  7.41e+01   8.12e+01]\n",
      " [  1.00e+01   9.02e+01]\n",
      " [  9.12e+01   1.00e+02]\n",
      " [  2.38e-01   5.53e-02]]\n",
      "Policy: [1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Q-learning example\n",
    "alpha = 1.  # learning rate\n",
    "gamma = 0.9 # discount factor\n",
    "epsilon = 0.1\n",
    "\n",
    "# tabular approximation\n",
    "Q = np.random.rand(n_states, n_actions)  # 5 states and 2 actions\n",
    "\n",
    "for e in range(20):\n",
    "    s = 0 \n",
    "    \n",
    "    for step in range(100):\n",
    "        a = e_greedy(Q, s, epsilon)  # greedy action selection\n",
    "        s1, r1 = transition(s, a)\n",
    "        \n",
    "        # TODO: update Q table here!\n",
    "        \n",
    "        print(\"s: \", s, \"a: \", \"s1:\", s1, \"a1:\", a1, \"Q: \", Q[s,a])\n",
    "        s = s1\n",
    "        if s == n_states-1:\n",
    "            break\n",
    "            \n",
    "print(\"Final Q: \", Q)\n",
    "print(\"Policy:\", np.argmax(Q, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
