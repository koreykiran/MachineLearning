


mu1 = [-1, -1]
cov1 = np.eye(2)

mu2 = [2,3]
cov2 = np.eye(2) * 3

C1 = np.random.multivariate_normal(mu1, cov1, 50)
C2 = np.random.multivariate_normal(mu2, cov2, 50)

plt.plot(C1[:, 0], C1[:, 1], 'or')
plt.plot(C2[:, 0], C2[:, 1], 'xb')

plt.xlim([-3, 6])
plt.ylim([-3, 7])

X = np.vstack((C1, C2))
T = np.ones(100)
T[:50] *= -1

# Train and Test data
N1 = C1.shape[0]
N2 = C2.shape[0]
N = N1 + N2

Xtrain = np.vstack((C1, C2))
Ttrain = np.ones(100)
Ttrain[:N1] *= -1

prior1 = N1 / N
prior2 = N2 / N

## now compute the discriminant function on test data

lda =LDA()
lda.train(Xtrain,Ttrain)

xs, ys = np.meshgrid(np.linspace(-3,6, 500), np.linspace(-3,7, 500))
Xtest = np.vstack((xs.flat, ys.flat)).T

d = lda.use(Xtest)
print("d1=",d[0])
print("d2=",d[1])

fig = plt.figure(figsize=(8,8))
ax = fig.gca(projection='3d')
ax.plot_surface(xs, ys, d[0].reshape(xs.shape), alpha=0.2)
ax.plot_surface(xs, ys, d[1].reshape(xs.shape), alpha=0.4)
plt.title("QDA Discriminant Functions")

plt.figure(figsize=(6,6))
plt.contourf(xs, ys, (d[0]-d[1] > 0).reshape(xs.shape))
#print("db",(d[0]-d[1] > 0).reshape(xs.shape))
plt.title("Decision Boundary")
plt.plot(C1[:, 0], C1[:, 1], 'or')
plt.plot(C2[:, 0], C2[:, 1], 'xb')

# # Plot generative distributions  p(x | Class=k)  starting with discriminant functions

# fig = plt.figure(figsize=(8,8))
# ax = fig.gca(projection='3d')

# prob1 = np.exp( d1.reshape(xs.shape) - 0.5*X.shape[1]*np.log(2*np.pi) - np.log(prior1))
# prob2 = np.exp( d2.reshape(xs.shape) - 0.5*X.shape[1]*np.log(2*np.pi) - np.log(prior2))
# ax.plot_surface(xs, ys, prob1, alpha=0.2)
# ax.plot_surface(xs, ys, prob2, alpha=0.4)

# plt.ylabel("QDA P(x|Class=k)\n from disc funcs", multialignment="center")